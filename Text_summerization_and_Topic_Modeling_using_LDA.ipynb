{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm_notebook\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from gensim.models import Phrases\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "\n",
    "It is a dataset of articles taken from BBCâ€™s website. The csv file contains the articles and its corresponding language in two seperate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('articles_bbc_2018_01_30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the rows containing empty cells\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Image copyright PA/EPA Image caption Oligarch ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Husband admits killing French jogger\\r\\n\\r\\nTh...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Manchester City's Leroy Sane is ruled out for ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Image copyright AFP Image caption Sebastien Br...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles lang\n",
       "0  Image copyright PA/EPA Image caption Oligarch ...   en\n",
       "1  Husband admits killing French jogger\\r\\n\\r\\nTh...   en\n",
       "2  Media playback is unsupported on your device M...   en\n",
       "3  Manchester City's Leroy Sane is ruled out for ...   en\n",
       "4  Image copyright AFP Image caption Sebastien Br...   en"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Keeping English articles only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112d1e561eb3487bb3bfcdcde8c2455b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMSM-TECH\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1e1323e62e4d80bf7f928d246f83dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=308), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tqdm_notebook().pandas()\n",
    "# Detecting the language of all the articles using detect function and put it the the column lang\n",
    "data['lang'] = data.articles.progress_map(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    257\n",
       "fa      9\n",
       "fr      7\n",
       "id      5\n",
       "uk      4\n",
       "ar      4\n",
       "hi      4\n",
       "vi      4\n",
       "ru      4\n",
       "sw      3\n",
       "tr      2\n",
       "es      2\n",
       "pt      2\n",
       "de      1\n",
       "Name: lang, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of articles in each language\n",
    "data.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the articles in english only\n",
    "data = data.loc[data.lang=='en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae411de1a24240f992a292294c662549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the articles into sentences in 'sentences' column\n",
    "data['sentences'] = data.articles.progress_map(sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\\r\\n\\r\\nRussian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.',\n",
       " 'The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.',\n",
       " 'However, the US stressed those named were not subject to new sanctions.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first 3 sentences of the 1st article\n",
    "data['sentences'].head(1).tolist()[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd55f9283ac491e9f68b52f1f648fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the sentences into words/tokens in 'tokens_sentences' column\n",
    "data['tokens_sentences'] = data['sentences'].progress_map(lambda sentences: [word_tokenize(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Image', 'copyright', 'PA/EPA', 'Image', 'caption', 'Oligarch', 'Roman', 'Abramovich', '(', 'l', ')', 'and', 'PM', 'Dmitry', 'Medvedev', 'are', 'on', 'the', 'list', 'Russian', 'President', 'Vladimir', 'Putin', 'says', 'a', 'list', 'of', 'officials', 'and', 'businessmen', 'close', 'to', 'the', 'Kremlin', 'published', 'by', 'the', 'US', 'has', 'in', 'effect', 'targeted', 'all', 'Russian', 'people', '.'], ['The', 'list', 'names', '210', 'top', 'Russians', 'as', 'part', 'of', 'a', 'sanctions', 'law', 'aimed', 'at', 'punishing', 'Moscow', 'for', 'meddling', 'in', 'the', 'US', 'election', '.'], ['However', ',', 'the', 'US', 'stressed', 'those', 'named', 'were', 'not', 'subject', 'to', 'new', 'sanctions', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Print the first 3 sentences of the 1st article after word tokenizations\n",
    "print(data['tokens_sentences'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Lemmatizing with POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging of words/tokens in all sentences of each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4556fe5cf049efb8eb51f13c684e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Using pos tagging before lemmtization and save it in 'POS_tokens' column\n",
    "data['POS_tokens'] = data['tokens_sentences'].progress_map(lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Image', 'NN'), ('copyright', 'NN'), ('PA/EPA', 'NNP'), ('Image', 'NNP'), ('caption', 'NN'), ('Oligarch', 'NNP'), ('Roman', 'NNP'), ('Abramovich', 'NNP'), ('(', '('), ('l', 'NN'), (')', ')'), ('and', 'CC'), ('PM', 'NNP'), ('Dmitry', 'NNP'), ('Medvedev', 'NNP'), ('are', 'VBP'), ('on', 'IN'), ('the', 'DT'), ('list', 'NN'), ('Russian', 'NNP'), ('President', 'NNP'), ('Vladimir', 'NNP'), ('Putin', 'NNP'), ('says', 'VBZ'), ('a', 'DT'), ('list', 'NN'), ('of', 'IN'), ('officials', 'NNS'), ('and', 'CC'), ('businessmen', 'NNS'), ('close', 'RB'), ('to', 'TO'), ('the', 'DT'), ('Kremlin', 'NNP'), ('published', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('has', 'VBZ'), ('in', 'IN'), ('effect', 'NN'), ('targeted', 'VBN'), ('all', 'DT'), ('Russian', 'JJ'), ('people', 'NNS'), ('.', '.')], [('The', 'DT'), ('list', 'NN'), ('names', 'RB'), ('210', 'CD'), ('top', 'JJ'), ('Russians', 'NNPS'), ('as', 'IN'), ('part', 'NN'), ('of', 'IN'), ('a', 'DT'), ('sanctions', 'NNS'), ('law', 'NN'), ('aimed', 'VBN'), ('at', 'IN'), ('punishing', 'VBG'), ('Moscow', 'NNP'), ('for', 'IN'), ('meddling', 'VBG'), ('in', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('election', 'NN'), ('.', '.')], [('However', 'RB'), (',', ','), ('the', 'DT'), ('US', 'NNP'), ('stressed', 'VBD'), ('those', 'DT'), ('named', 'VBN'), ('were', 'VBD'), ('not', 'RB'), ('subject', 'JJ'), ('to', 'TO'), ('new', 'JJ'), ('sanctions', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "# Printing the Pos tag output of the first 3 sentences of the 1st article \n",
    "print(data['POS_tokens'].head(1).tolist()[0][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes the tag obtained by POS tag and returns a wordnet tag to be used by the lemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6486c08f584b5588163fd3b716f320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=257), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizing each word with its POS tag, in each sentence and saving lemmatized words in 'tokens_sentences_lemmatized' column\n",
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].progress_map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1]))\n",
    "            if get_wordnet_pos(el[1]) != '' else el[0]\n",
    "            for el in tokens_POS\n",
    "        ] \n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Image',\n",
       "  'copyright',\n",
       "  'PA/EPA',\n",
       "  'Image',\n",
       "  'caption',\n",
       "  'Oligarch',\n",
       "  'Roman',\n",
       "  'Abramovich',\n",
       "  '(',\n",
       "  'l',\n",
       "  ')',\n",
       "  'and',\n",
       "  'PM',\n",
       "  'Dmitry',\n",
       "  'Medvedev',\n",
       "  'be',\n",
       "  'on',\n",
       "  'the',\n",
       "  'list',\n",
       "  'Russian',\n",
       "  'President',\n",
       "  'Vladimir',\n",
       "  'Putin',\n",
       "  'say',\n",
       "  'a',\n",
       "  'list',\n",
       "  'of',\n",
       "  'official',\n",
       "  'and',\n",
       "  'businessmen',\n",
       "  'close',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Kremlin',\n",
       "  'publish',\n",
       "  'by',\n",
       "  'the',\n",
       "  'US',\n",
       "  'have',\n",
       "  'in',\n",
       "  'effect',\n",
       "  'target',\n",
       "  'all',\n",
       "  'Russian',\n",
       "  'people',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'list',\n",
       "  'names',\n",
       "  '210',\n",
       "  'top',\n",
       "  'Russians',\n",
       "  'as',\n",
       "  'part',\n",
       "  'of',\n",
       "  'a',\n",
       "  'sanction',\n",
       "  'law',\n",
       "  'aim',\n",
       "  'at',\n",
       "  'punish',\n",
       "  'Moscow',\n",
       "  'for',\n",
       "  'meddle',\n",
       "  'in',\n",
       "  'the',\n",
       "  'US',\n",
       "  'election',\n",
       "  '.'],\n",
       " ['However',\n",
       "  ',',\n",
       "  'the',\n",
       "  'US',\n",
       "  'stress',\n",
       "  'those',\n",
       "  'name',\n",
       "  'be',\n",
       "  'not',\n",
       "  'subject',\n",
       "  'to',\n",
       "  'new',\n",
       "  'sanction',\n",
       "  '.']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the lemmatization output of the first 3 sentences of the 1st article \n",
    "data['tokens_sentences_lemmatized'].head(1).tolist()[0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Regrouping tokens and removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common thing with LDA is that words appear in multiple topics. One way to cope with this is to add these words to your stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our stopwards list\n",
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use', 'would', 'can']\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('English') + stopwords_verbs + stopwords_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of sentences of tokens into a list of tokens\n",
    "# and saving them in 'tokens' column\n",
    "data['tokens'] = data['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image',\n",
       " 'copyright',\n",
       " 'PA/EPA',\n",
       " 'Image',\n",
       " 'caption',\n",
       " 'Oligarch',\n",
       " 'Roman',\n",
       " 'Abramovich',\n",
       " '(',\n",
       " 'l',\n",
       " ')',\n",
       " 'and',\n",
       " 'PM',\n",
       " 'Dmitry',\n",
       " 'Medvedev',\n",
       " 'be',\n",
       " 'on',\n",
       " 'the',\n",
       " 'list',\n",
       " 'Russian',\n",
       " 'President',\n",
       " 'Vladimir',\n",
       " 'Putin',\n",
       " 'say',\n",
       " 'a',\n",
       " 'list',\n",
       " 'of',\n",
       " 'official',\n",
       " 'and',\n",
       " 'businessmen']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 3 sentences of the 1st article after flatten \n",
    "data['tokens'].head(1).tolist()[0][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower casing the lemmatized tokens, removing stop words, make sure that the length is more than one \n",
    "# and make sure that each token has only alphabetic characters(removing tokens with punctuation or Removing words with digits)\n",
    "data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha() \n",
    "                                                    and token.lower() not in my_stopwords and len(token)>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oligarch',\n",
       " 'roman',\n",
       " 'abramovich',\n",
       " 'pm',\n",
       " 'dmitry',\n",
       " 'medvedev',\n",
       " 'list',\n",
       " 'russian',\n",
       " 'president',\n",
       " 'vladimir',\n",
       " 'putin',\n",
       " 'list',\n",
       " 'official',\n",
       " 'businessmen',\n",
       " 'close',\n",
       " 'kremlin',\n",
       " 'publish',\n",
       " 'us',\n",
       " 'effect',\n",
       " 'target',\n",
       " 'russian',\n",
       " 'people',\n",
       " 'list',\n",
       " 'names',\n",
       " 'top',\n",
       " 'russians',\n",
       " 'part',\n",
       " 'sanction',\n",
       " 'law',\n",
       " 'aim']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the first 3 sentences of the 1st article\n",
    "data['tokens'].head(1).tolist()[0][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA \n",
    "\n",
    "LDA (short for Latent Dirichlet Allocation) is an unsupervised machine-learning model that takes documents as input and finds topics as output. The model also says in what percentage each document talks about each topic.\n",
    "A topic is represented as a weighted list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Prepare bi-grams and tri-grams\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. This step is useful to grasp more relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data['tokens'].tolist()\n",
    "bigram_model = Phrases(tokens)\n",
    "trigram_model = Phrases(bigram_model[tokens], min_count=1)\n",
    "tokens = list(trigram_model[bigram_model[tokens]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Prepare objects for LDA gensim implementation\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary and the corpus(in bag of words form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(tokens)\n",
    "#Filter out tokens that appear inless than 3 documents\n",
    "dictionary_LDA.filter_extremes(no_below=3) \n",
    "corpus = [dictionary_LDA.doc2bow(tok) for tok in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA implementation and running for 4 epochs\n",
    "\n",
    "The other important parameters of the model are:\n",
    "- the number of topics is equal to num_topics\n",
    "- the number of words per topic is handled by eta\n",
    "- the number of topics per document is handled by alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 20 \n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) printing the topics\n",
    "\n",
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage (importance) to the topic. all the 20 topics are printed with their first 20 most relevant words the weightage of each keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.012*\"specie\" + 0.010*\"prey\" + 0.007*\"act\" + 0.006*\"animal\" + 0.006*\"help\" + 0.005*\"back\" + 0.005*\"find\" + 0.005*\"become\" + 0.005*\"behaviour\" + 0.005*\"number\" + 0.005*\"area\" + 0.005*\"include\" + 0.004*\"order\" + 0.004*\"kill\" + 0.004*\"approach\" + 0.004*\"however\" + 0.004*\"move\" + 0.004*\"host\" + 0.004*\"evolve\" + 0.004*\"well\"\n",
      "\n",
      "1: 0.010*\"show\" + 0.008*\"game\" + 0.007*\"another\" + 0.007*\"light\" + 0.007*\"night\" + 0.006*\"find\" + 0.006*\"could\" + 0.006*\"predator\" + 0.006*\"animal\" + 0.006*\"give\" + 0.006*\"images\" + 0.006*\"question\" + 0.006*\"however\" + 0.006*\"group\" + 0.006*\"live\" + 0.006*\"time\" + 0.005*\"transparent\" + 0.005*\"drug\" + 0.005*\"eye\" + 0.005*\"call\"\n",
      "\n",
      "2: 0.006*\"first\" + 0.006*\"people\" + 0.006*\"work\" + 0.005*\"city\" + 0.005*\"time\" + 0.005*\"year\" + 0.004*\"day\" + 0.004*\"part\" + 0.004*\"find\" + 0.004*\"could\" + 0.004*\"us\" + 0.003*\"run\" + 0.003*\"restaurant\" + 0.003*\"plan\" + 0.003*\"home\" + 0.003*\"start\" + 0.003*\"tunnel\" + 0.003*\"include\" + 0.003*\"bring\" + 0.003*\"dish\"\n",
      "\n",
      "3: 0.014*\"separatist\" + 0.011*\"people\" + 0.008*\"die\" + 0.008*\"call\" + 0.007*\"coalition\" + 0.007*\"northern_mali\" + 0.007*\"ansar_dine\" + 0.007*\"city\" + 0.006*\"force\" + 0.006*\"play\" + 0.006*\"islamist\" + 0.005*\"government\" + 0.005*\"around\" + 0.005*\"victim\" + 0.005*\"steal\" + 0.005*\"talk\" + 0.005*\"year\" + 0.005*\"prime_minister\" + 0.005*\"show\" + 0.005*\"add\"\n",
      "\n",
      "4: 0.008*\"people\" + 0.007*\"wave\" + 0.006*\"tell\" + 0.006*\"town\" + 0.006*\"water\" + 0.006*\"ocean\" + 0.005*\"benefit\" + 0.005*\"village\" + 0.005*\"world\" + 0.005*\"back\" + 0.005*\"leave\" + 0.004*\"us\" + 0.004*\"time\" + 0.004*\"fin\" + 0.004*\"work\" + 0.004*\"man\" + 0.004*\"look\" + 0.004*\"way\" + 0.004*\"first\" + 0.004*\"city\"\n",
      "\n",
      "5: 0.022*\"dutch\" + 0.011*\"issue\" + 0.010*\"border\" + 0.010*\"news\" + 0.009*\"become\" + 0.009*\"israel\" + 0.008*\"netherlands\" + 0.008*\"agency\" + 0.008*\"story\" + 0.007*\"report\" + 0.007*\"service\" + 0.007*\"un\" + 0.007*\"several\" + 0.006*\"international\" + 0.006*\"might\" + 0.006*\"part\" + 0.006*\"shy\" + 0.005*\"output\" + 0.005*\"problem\" + 0.005*\"local\"\n",
      "\n",
      "6: 0.012*\"event\" + 0.011*\"people\" + 0.009*\"president\" + 0.009*\"digital\" + 0.008*\"authority\" + 0.008*\"show\" + 0.007*\"news\" + 0.007*\"end\" + 0.007*\"election\" + 0.007*\"supporter\" + 0.007*\"correspondent\" + 0.006*\"capital\" + 0.006*\"grow\" + 0.006*\"still\" + 0.006*\"report\" + 0.006*\"well\" + 0.006*\"leave\" + 0.006*\"national\" + 0.006*\"film\" + 0.005*\"programme\"\n",
      "\n",
      "7: 0.011*\"news\" + 0.010*\"ant\" + 0.008*\"twitter\" + 0.007*\"single\" + 0.007*\"live\" + 0.006*\"name\" + 0.006*\"specie\" + 0.006*\"small\" + 0.006*\"people\" + 0.006*\"story\" + 0.005*\"welcome\" + 0.005*\"mate\" + 0.005*\"effort\" + 0.005*\"alert\" + 0.005*\"receive\" + 0.005*\"power\" + 0.005*\"millipede\" + 0.005*\"consider\" + 0.005*\"pollution\" + 0.005*\"tiny\"\n",
      "\n",
      "8: 0.012*\"us\" + 0.008*\"russia\" + 0.007*\"could\" + 0.007*\"time\" + 0.007*\"write\" + 0.006*\"president\" + 0.006*\"rocket\" + 0.006*\"call\" + 0.006*\"satellite\" + 0.006*\"trump\" + 0.005*\"many\" + 0.005*\"people\" + 0.005*\"raise\" + 0.005*\"flight\" + 0.005*\"set\" + 0.005*\"political\" + 0.005*\"tell\" + 0.004*\"try\" + 0.004*\"job\" + 0.004*\"mission\"\n",
      "\n",
      "9: 0.014*\"case\" + 0.010*\"male\" + 0.009*\"review\" + 0.009*\"prey\" + 0.008*\"find\" + 0.007*\"evidence\" + 0.007*\"kick\" + 0.007*\"message\" + 0.007*\"live\" + 0.006*\"life\" + 0.006*\"place\" + 0.006*\"study\" + 0.006*\"predator\" + 0.006*\"give\" + 0.006*\"part\" + 0.006*\"people\" + 0.006*\"number\" + 0.005*\"mate\" + 0.005*\"put\" + 0.004*\"store\"\n",
      "\n",
      "10: 0.009*\"people\" + 0.006*\"government\" + 0.006*\"uk\" + 0.006*\"think\" + 0.006*\"brexit\" + 0.005*\"tell\" + 0.005*\"time\" + 0.005*\"could\" + 0.005*\"us\" + 0.005*\"work\" + 0.004*\"even\" + 0.004*\"find\" + 0.004*\"minister\" + 0.004*\"day\" + 0.004*\"already\" + 0.004*\"help\" + 0.004*\"test\" + 0.004*\"bee\" + 0.004*\"still\" + 0.004*\"back\"\n",
      "\n",
      "11: 0.015*\"us\" + 0.014*\"stress\" + 0.011*\"healthcare\" + 0.008*\"day\" + 0.008*\"sleep\" + 0.007*\"could\" + 0.007*\"president\" + 0.007*\"test\" + 0.007*\"find\" + 0.007*\"keep\" + 0.006*\"put\" + 0.006*\"try\" + 0.006*\"images\" + 0.006*\"amazon\" + 0.006*\"provide\" + 0.006*\"team\" + 0.005*\"benefit\" + 0.005*\"expert\" + 0.005*\"body\" + 0.005*\"meeting\"\n",
      "\n",
      "12: 0.017*\"work\" + 0.013*\"people\" + 0.011*\"call\" + 0.008*\"model\" + 0.007*\"help\" + 0.007*\"find\" + 0.007*\"woman\" + 0.006*\"sleep\" + 0.006*\"spanish\" + 0.006*\"age\" + 0.006*\"radio\" + 0.006*\"happen\" + 0.006*\"report\" + 0.005*\"well\" + 0.005*\"add\" + 0.005*\"include\" + 0.005*\"accord\" + 0.005*\"police\" + 0.005*\"much\" + 0.005*\"job\"\n",
      "\n",
      "13: 0.034*\"video\" + 0.021*\"logo\" + 0.020*\"design\" + 0.015*\"platform\" + 0.014*\"tourism\" + 0.012*\"app\" + 0.012*\"animal\" + 0.012*\"country\" + 0.011*\"respect\" + 0.010*\"new\" + 0.009*\"include\" + 0.009*\"device\" + 0.009*\"call\" + 0.007*\"content\" + 0.007*\"around\" + 0.006*\"home\" + 0.006*\"others\" + 0.006*\"key\" + 0.006*\"television\" + 0.006*\"news_app\"\n",
      "\n",
      "14: 0.017*\"us\" + 0.007*\"time\" + 0.006*\"first\" + 0.006*\"america\" + 0.006*\"year\" + 0.006*\"product\" + 0.005*\"find\" + 0.005*\"amazon\" + 0.005*\"company\" + 0.005*\"still\" + 0.005*\"call\" + 0.005*\"president\" + 0.004*\"customer\" + 0.004*\"live\" + 0.004*\"store\" + 0.004*\"trump\" + 0.004*\"even\" + 0.004*\"put\" + 0.004*\"back\" + 0.004*\"light\"\n",
      "\n",
      "15: 0.014*\"list\" + 0.012*\"data\" + 0.012*\"us\" + 0.011*\"publish\" + 0.010*\"name\" + 0.010*\"could\" + 0.010*\"base\" + 0.009*\"study\" + 0.009*\"human\" + 0.007*\"russian\" + 0.007*\"security\" + 0.006*\"work\" + 0.006*\"journalist\" + 0.006*\"report\" + 0.006*\"activity\" + 0.005*\"level\" + 0.005*\"think\" + 0.005*\"system\" + 0.005*\"add\" + 0.005*\"afghanistan\"\n",
      "\n",
      "16: 0.009*\"find\" + 0.008*\"light\" + 0.008*\"us\" + 0.007*\"people\" + 0.006*\"work\" + 0.006*\"help\" + 0.005*\"day\" + 0.005*\"much\" + 0.005*\"think\" + 0.005*\"year\" + 0.005*\"images\" + 0.005*\"first\" + 0.005*\"actor\" + 0.005*\"write\" + 0.005*\"africa\" + 0.005*\"company\" + 0.004*\"right\" + 0.004*\"feel\" + 0.004*\"give\" + 0.004*\"week\"\n",
      "\n",
      "17: 0.010*\"idea\" + 0.008*\"work\" + 0.008*\"link\" + 0.007*\"news\" + 0.007*\"find\" + 0.007*\"world\" + 0.007*\"fast\" + 0.006*\"place\" + 0.006*\"information\" + 0.005*\"note\" + 0.005*\"start\" + 0.005*\"content\" + 0.005*\"chris\" + 0.005*\"audience\" + 0.005*\"human\" + 0.005*\"body\" + 0.005*\"include\" + 0.004*\"pick\" + 0.004*\"material\" + 0.004*\"journalism\"\n",
      "\n",
      "18: 0.020*\"eu\" + 0.008*\"tell\" + 0.008*\"government\" + 0.007*\"europe\" + 0.007*\"country\" + 0.006*\"visit\" + 0.006*\"include\" + 0.006*\"brexit\" + 0.006*\"work\" + 0.006*\"day\" + 0.006*\"new\" + 0.006*\"flight\" + 0.005*\"travel\" + 0.005*\"two\" + 0.005*\"deal\" + 0.005*\"vision\" + 0.005*\"couple\" + 0.005*\"reuters\" + 0.004*\"part\" + 0.004*\"route\"\n",
      "\n",
      "19: 0.026*\"video\" + 0.019*\"http\" + 0.017*\"drama\" + 0.013*\"set\" + 0.013*\"code\" + 0.011*\"always\" + 0.011*\"reference\" + 0.010*\"thank\" + 0.010*\"music\" + 0.009*\"listen\" + 0.009*\"young\" + 0.009*\"decade\" + 0.008*\"could\" + 0.008*\"live\" + 0.008*\"think\" + 0.008*\"end\" + 0.008*\"state\" + 0.008*\"love\" + 0.008*\"show\" + 0.008*\"receive\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Allocating topics to documents\n",
    " \n",
    "We can print the percentage of topics a document is about as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image copyright PA/EPA Image caption Oligarch Roman Abramovich (l) and PM Dmitry Medvedev are on the list\n",
      "\n",
      "Russian President Vladimir Putin says a list of officials and businessmen close to the Kremlin published by the US has in effect targeted all Russian people.\n",
      "\n",
      "The list names 210 top Russians as part of a sanctions law aimed at punishing Moscow for meddling in the US election.\n",
      "\n",
      "However, the US stressed those named were not subject to new sanctions.\n",
      "\n",
      "Mr Putin said the list was an unfriendly act that complicated US-Russia ties but he said he did not want to escalate the situation.\n",
      "\n",
      "Mr Putin said Russia should instead be thinking about \"ourselves and the economy\".\n",
      "\n",
      "The list was also derided by a number of senior Russian officials who said it bore a strong resemblance to the Forbes magazine ranking of Russian billionaires. A US Treasury Department later told Buzzfeed that an unclassified annex of the report had been derived from the magazine.\n",
      "\n",
      "Why did the US publish the list?\n",
      "\n",
      "The government was required to draw up the list after Congress passed the Countering America's Adversaries Through Sanctions Act (Caatsa) in August.\n",
      "\n",
      "The law aimed to punish Russia for its alleged meddling in the 2016 US presidential election and its actions in Ukraine.\n",
      "\n",
      "Congress wanted the list to name and shame those who had benefited from close association with President Putin and put them on notice that they could be targeted for sanctions, or more sanctions, in the future.\n",
      "\n",
      "President Donald Trump did not support Caatsa, even though he signed it into law, saying it was \"unconstitutional\".\n",
      "\n",
      "Under the law, the list had to be delivered by Monday. The fact it was released about 10 minutes before midnight may reflect Mr Trump's coolness towards it, and his opposition to punishing more Russians with sanctions.\n",
      "\n",
      "The top Democrat on the House Foreign Affairs Committee, Eliot Engel, accused the Trump administration of letting \"Russia off the hook again\" by not taking substantial action.\n",
      "\n",
      "Who has been named?\n",
      "\n",
      "Informally known as the \"Putin list\", the unclassified section has 210 names, 114 of them in the government or linked to it, or key businessmen. The other 96 are oligarchs apparently determined more by the fact they are worth more than $1bn (Â£710m) than their close ties to the Kremlin.\n",
      "\n",
      "Image copyright Reuters Image caption Congress passed the law in August, although President Donald Trump had opposed it\n",
      "\n",
      "Most of Mr Putin's longstanding allies are named, many of them siloviki (security guys). They include the spy chiefs Alexander Bortnikov of the Federal Security Service (FSB) - which Mr Putin used to run - and Sergei Naryshkin of the Foreign Intelligence Service (SVR).\n",
      "\n",
      "The men who control Russia's energy resources are listed: Gazprom chief Alexei Miller, Rosneft chief Igor Sechin and other oil and gas executives, along with top bankers like Bank Rossiya manager Yuri Kovalchuk.\n",
      "\n",
      "The oligarchs include Kirill Shamalov, who is reported to be Mr Putin's son-in-law, although the Kremlin has never confirmed his marriage to Katerina Tikhonova, nor even that she is the president's daughter.\n",
      "\n",
      "Internationally known oligarchs are there too, such as those with stakes in top English football clubs: Alisher Usmanov (Arsenal) and Roman Abramovich (Chelsea).\n",
      "\n",
      "Will they face new sanctions?\n",
      "\n",
      "Not at the moment. The US Treasury document itself stresses: \"It is not a sanctions list, and the inclusion of individuals or entities... does not and in no way should be interpreted to impose sanctions on those individuals or entities.\"\n",
      "\n",
      "It adds: \"Neither does inclusion on the unclassified list indicate that the US government has information about the individual's involvement in malign activities.\"\n",
      "\n",
      "However, there is a classified version said to include information detailing allegations of involvement in corrupt activities.\n",
      "\n",
      "What does it mean for Russia's elite?\n",
      "\n",
      "Analysis: Steve Rosenberg, BBC Moscow correspondent\n",
      "\n",
      "The good news for the Kremlin: this isn't a sanctions list. But the good news ends there.\n",
      "\n",
      "Those Russian officials and oligarchs named by the US Treasury will worry that their inclusion could signal sanctions in the future.\n",
      "\n",
      "Even before the list was made public, the Kremlin had claimed the US Treasury report was an attempt to meddle in Russia's presidential election.\n",
      "\n",
      "The list reads like a Who's Who of the Russian political elite and business world.\n",
      "\n",
      "Moscow won't want that to become a Who's Sanctioned.\n",
      "\n",
      "What is the Caatsa act and did the president want it?\n",
      "\n",
      "The law limited the amount of money Americans could invest in Russian energy projects and made it more difficult for US companies to do business with Russia.\n",
      "\n",
      "It also imposed sanctions on Iran and North Korea.\n",
      "\n",
      "Media playback is unsupported on your device Media caption All you need to know about the Trump-Russia investigation\n",
      "\n",
      "In signing the act, Mr Trump attached a statement calling the measure \"deeply flawed\" and said he could make \"far better deals with foreign countries than Congress\".\n",
      "\n",
      "Earlier on Monday, the US government argued the Caatsa law had already pushed governments around the world to cancel deals with Russia worth billions, suggesting that more sanctions were not required.\n",
      "\n",
      "How have the Russians reacted?\n",
      "\n",
      "Perhaps referring to the fact that all of their political representatives had been named, Mr Putin said that, in effect, \"all 146 million Russians have been put on the list\".\n",
      "\n",
      "He joked he was offended not to be named himself.\n",
      "\n",
      "Earlier, Kremlin spokesman Dmitry Peskov, who is himself on the list, accepted that it was not one of sanctions but said it could potentially damage \"the image and reputation\" of figures listed and their associated companies.\n",
      "\n",
      "He added: \"It's not the first day that we live with quite aggressive comments made towards us, so we should not give in to emotions.\"\n",
      "\n",
      "When Caatsa was passed, PM Dmitry Medvedev said it meant the US had declared a \"full-scale trade war\" on Russia.\n",
      "\n",
      "Russian opposition leader Alexei Navalny praised the publication of the names as \"a good list\".\n"
     ]
    }
   ],
   "source": [
    "# printing the first article\n",
    "print(data.articles.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 0.99830645)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicting the percentage of topics in the first article\n",
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that article 1 contains topic 15 with percentage 99.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Predicting topics on unseen documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = '''Eric Tucker, a 35-year-old co-founder of a marketing company in Austin, Tex., had just about 40 Twitter followers. But his recent tweet about paid protesters being bused to demonstrations against President-elect Donald J. Trump fueled a nationwide conspiracy theory â€” one that Mr. Trump joined in promoting. \n",
    "\n",
    "Mr. Tucker's post was shared at least 16,000 times on Twitter and more than 350,000 times on Facebook. The problem is that Mr. Tucker got it wrong. There were no such buses packed with paid protesters.\n",
    "\n",
    "But that didn't matter.\n",
    "\n",
    "While some fake news is produced purposefully by teenagers in the Balkans or entrepreneurs in the United States seeking to make money from advertising, false information can also arise from misinformed social media posts by regular people that are seized on and spread through a hyperpartisan blogosphere.\n",
    "\n",
    "Here, The New York Times deconstructs how Mr. Tuckerâ€™s now-deleted declaration on Twitter the night after the election turned into a fake-news phenomenon. It is an example of how, in an ever-connected world where speed often takes precedence over truth, an observation by a private citizen can quickly become a talking point, even as it is being proved false.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic #</th>\n",
       "      <th>weight</th>\n",
       "      <th>words in topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.006*\"first\" + 0.006*\"people\" + 0.006*\"work\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.012*\"us\" + 0.008*\"russia\" + 0.007*\"could\" + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.009*\"people\" + 0.006*\"government\" + 0.006*\"u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.010*\"idea\" + 0.008*\"work\" + 0.008*\"link\" + 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic #  weight                                     words in topic\n",
       "0        2    0.40  0.006*\"first\" + 0.006*\"people\" + 0.006*\"work\" ...\n",
       "1        8    0.22  0.012*\"us\" + 0.008*\"russia\" + 0.007*\"could\" + ...\n",
       "2       10    0.29  0.009*\"people\" + 0.006*\"government\" + 0.006*\"u...\n",
       "3       17    0.09  0.010*\"idea\" + 0.008*\"work\" + 0.008*\"link\" + 0..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving all topics and the corresponding keywords in a variable\n",
    "topics_all = lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=20)\n",
    "# tokenization of the new document\n",
    "tokens = word_tokenize(document)\n",
    "# creating the bag of words using the previously created dictionary\n",
    "bow_corpus=dictionary_LDA.doc2bow(tokens)\n",
    "# predicting the topics of the document using the LDA model\n",
    "topics_pred = lda_model[bow_corpus]\n",
    "# creating a dataframe contains the predicted topics in the doc, the corresponding percentage of each topic \n",
    "# as well as the keywords of this topic\n",
    "pd.DataFrame([(el[0], round(el[1],2), topics_all[el[0]][1]) for el in topics_pred], columns=['topic #', 'weight', 'words in topic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Finding the dominant topic in each sentence\n",
    "\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "The format_topics_sentences() function below nicely aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>list, data, us, publish, name, could, base, st...</td>\n",
       "      <td>Image copyright PA/EPA Image caption Oligarch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>us, russia, could, time, write, president, roc...</td>\n",
       "      <td>Husband admits killing French jogger\\r\\n\\r\\nTh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9488</td>\n",
       "      <td>separatist, people, die, call, coalition, nort...</td>\n",
       "      <td>Media playback is unsupported on your device M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>us, russia, could, time, write, president, roc...</td>\n",
       "      <td>Manchester City's Leroy Sane is ruled out for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>first, people, work, city, time, year, day, pa...</td>\n",
       "      <td>Image copyright AFP Image caption Sebastien Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>find, light, us, people, work, help, day, much...</td>\n",
       "      <td>The middle of nowhere\\r\\n\\r\\nFive miles from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>us, russia, could, time, write, president, roc...</td>\n",
       "      <td>Image copyright Reuters Image caption Mr Trump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.7989</td>\n",
       "      <td>list, data, us, publish, name, could, base, st...</td>\n",
       "      <td>Putin says US list targets all Russians\\r\\n\\r\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8417</td>\n",
       "      <td>first, people, work, city, time, year, day, pa...</td>\n",
       "      <td>Image copyright Getty Images\\r\\n\\r\\nIt is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>people, government, uk, think, brexit, tell, t...</td>\n",
       "      <td>Image copyright Reuters Image caption The high...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0            15.0              0.9983   \n",
       "1            1             8.0              0.9768   \n",
       "2            2             3.0              0.9488   \n",
       "3            3             8.0              0.9736   \n",
       "4            4             2.0              0.9990   \n",
       "5            5            16.0              0.9969   \n",
       "6            6             8.0              0.9793   \n",
       "7            7            15.0              0.7989   \n",
       "8            8             2.0              0.8417   \n",
       "9            9            10.0              0.9983   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  list, data, us, publish, name, could, base, st...   \n",
       "1  us, russia, could, time, write, president, roc...   \n",
       "2  separatist, people, die, call, coalition, nort...   \n",
       "3  us, russia, could, time, write, president, roc...   \n",
       "4  first, people, work, city, time, year, day, pa...   \n",
       "5  find, light, us, people, work, help, day, much...   \n",
       "6  us, russia, could, time, write, president, roc...   \n",
       "7  list, data, us, publish, name, could, base, st...   \n",
       "8  first, people, work, city, time, year, day, pa...   \n",
       "9  people, government, uk, think, brexit, tell, t...   \n",
       "\n",
       "                                                Text  \n",
       "0  Image copyright PA/EPA Image caption Oligarch ...  \n",
       "1  Husband admits killing French jogger\\r\\n\\r\\nTh...  \n",
       "2  Media playback is unsupported on your device M...  \n",
       "3  Manchester City's Leroy Sane is ruled out for ...  \n",
       "4  Image copyright AFP Image caption Sebastien Br...  \n",
       "5  The middle of nowhere\\r\\n\\r\\nFive miles from t...  \n",
       "6  Image copyright Reuters Image caption Mr Trump...  \n",
       "7  Putin says US list targets all Russians\\r\\n\\r\\...  \n",
       "8  Image copyright Getty Images\\r\\n\\r\\nIt is the ...  \n",
       "9  Image copyright Reuters Image caption The high...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data.articles):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data.articles)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Find the most representative document for each topic (Summeriation)\n",
    "\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>specie, prey, act, animal, help, back, find, b...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9991</td>\n",
       "      <td>show, game, another, light, night, find, could...</td>\n",
       "      <td>Image copyright Square Enix Image caption Life...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>first, people, work, city, time, year, day, pa...</td>\n",
       "      <td>Series 4\\r\\n\\r\\nUrsula and Fiona: I Never Got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>separatist, people, die, call, coalition, nort...</td>\n",
       "      <td>That Was Then\\r\\n\\r\\nEpisode 5\\r\\n\\r\\nUncover ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>people, wave, tell, town, water, ocean, benefi...</td>\n",
       "      <td>Boring Talks #02 - Book Pricing Algorithms\\r\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.9987   \n",
       "1        1.0              0.9991   \n",
       "2        2.0              0.9995   \n",
       "3        3.0              0.9883   \n",
       "4        4.0              0.9993   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  specie, prey, act, animal, help, back, find, b...   \n",
       "1  show, game, another, light, night, find, could...   \n",
       "2  first, people, work, city, time, year, day, pa...   \n",
       "3  separatist, people, die, call, coalition, nort...   \n",
       "4  people, wave, tell, town, water, ocean, benefi...   \n",
       "\n",
       "                                                Text  \n",
       "0                                                NaN  \n",
       "1  Image copyright Square Enix Image caption Life...  \n",
       "2  Series 4\\r\\n\\r\\nUrsula and Fiona: I Never Got ...  \n",
       "3  That Was Then\\r\\n\\r\\nEpisode 5\\r\\n\\r\\nUncover ...  \n",
       "4  Boring Talks #02 - Book Pricing Algorithms\\r\\n...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
